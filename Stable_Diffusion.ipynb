{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariap2021/Bayesiana/blob/main/Stable_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11548128",
      "metadata": {},
      "source": [
        "## Activities  Team 9\n",
        "\n",
        "- Manuela Lopera Maya\n",
        "- Maria Del Pilar Mira Londo√±o\n",
        "- Pablo Restrepo\n",
        "\n",
        "1. Create your own image with a nifty prompt (free topic)\n",
        "2. Take a picture with your picture and modified it with a optimized prompt (free topic) -> E.g. Changes of environments or style, remove background, etc...\n",
        "3. Make your own video with a sequence prompt (free topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0391427a-1638-4338-8712-028d65c929c1",
      "metadata": {
        "id": "0391427a-1638-4338-8712-028d65c929c1",
        "outputId": "7e40bdd0-1e06-4026-ae7d-26005002eae6"
      },
      "outputs": [],
      "source": [
        "pip install -U diffusers accelerate transformers huggingface_hub imageio[ffmpeg] safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74459a63",
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -U \"transformers<5\" \"huggingface_hub<1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9895615a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import diffusers\n",
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6399e9e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "387146d5-537c-4eaa-92c3-9700ffbc67f4",
      "metadata": {
        "id": "387146d5-537c-4eaa-92c3-9700ffbc67f4"
      },
      "outputs": [],
      "source": [
        "import json #Create your file as AUTH.json -> {\"API_KEY\":\"My API KEY\"}\n",
        "with open('./AUTH.json', 'r') as file:\n",
        "    creds = json.load(file) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "82cc70fe-3854-45c3-b75c-9400a5121b61",
      "metadata": {
        "id": "82cc70fe-3854-45c3-b75c-9400a5121b61"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Put here your personal token -> login(\"MY_API_KEY\")\n",
        "login(creds[\"API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7148c996-4194-4be9-9dc4-22ee44279936",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "6211314e22f34d0fb43d81647d250a25"
          ]
        },
        "id": "7148c996-4194-4be9-9dc4-22ee44279936",
        "outputId": "d83ca01e-3b32-40c4-a915-b966055b0feb"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe_1 = StableDiffusionPipeline.from_pretrained(\n",
        "    \"sd-legacy/stable-diffusion-v1-5\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f33fef-bc57-4f73-9dc2-31058f2c04ae",
      "metadata": {
        "id": "d0f33fef-bc57-4f73-9dc2-31058f2c04ae"
      },
      "source": [
        "## Activities\n",
        "\n",
        "1. Create your own image with a nifty prompt (free topic)\n",
        "2. Take a picture with your picture and modified it with a optimized prompt (free topic) -> E.g. Changes of environments or style, remove background, etc...\n",
        "3. Make your own video with a sequence prompt (free topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8fa19a-b2ed-48ee-8d93-84cd33f7c396",
      "metadata": {
        "id": "ea8fa19a-b2ed-48ee-8d93-84cd33f7c396"
      },
      "source": [
        "## üñºÔ∏è 1. Text-to-Image Generation ‚Äì Sunset in Kenya ü¶í\n",
        "\n",
        "For this image, a prompt was used to describe a sunset in the Kenyan savannah, featuring orange and pink skies, a lone giraffe on the horizon, and silhouetted acacia trees. The scene is generated in a realistic wildlife photography style, emphasizing warm lighting and a calm atmosphere.\n",
        "\n",
        "The result is a clear and balanced image of an African sunset, created entirely from a text description.\n",
        "\n",
        "<img src=\"kenia.png\" alt=\"Sunset in Kenia\" width=\"300\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "08f4412f-9d0e-4a11-8beb-a184f05a1639",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f488bf55e3514f168ae8dcee773a6b56"
          ]
        },
        "id": "08f4412f-9d0e-4a11-8beb-a184f05a1639",
        "outputId": "cffd4318-efb0-4dab-bac5-63003115ffbc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67e2e08f60e64eeda4c09246a7b46fa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Generate image\n",
        "\n",
        "prompt = (\n",
        "\"A stunning sunset in the savannah of Kenya, with vibrant orange and pink skies, a lone giraffe standing gracefully on the horizon, tall acacia trees silhouetted, warm and serene atmosphere, realistic wildlife photography style\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "image = pipe_1(prompt, num_inference_steps=80).images[0]\n",
        "\n",
        "# Show and save image\n",
        "image.show()\n",
        "image.save(\"kenia.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fc4f741e-4de6-4360-8380-5ccd2a63c966",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "aae084b852f4415d95d9caaeb6d54576",
            "e7c9389e44ea48e2b2d0d3f6a86c13ea"
          ]
        },
        "id": "fc4f741e-4de6-4360-8380-5ccd2a63c966",
        "outputId": "875a50cd-7d70-462a-f48b-af280838d616"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e42e58dd7ce34f0fa22e03a31b167c5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "pipe_2 = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
        "    \"timbrooks/instruct-pix2pix\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "799a2a90-029e-4c53-bbd6-1622ac461abd",
      "metadata": {
        "id": "799a2a90-029e-4c53-bbd6-1622ac461abd"
      },
      "source": [
        "\n",
        "\n",
        "## üñºÔ∏èImage-to-Image Enhancement ‚Äì *Turning Glasses into a Product Shot*\n",
        "\n",
        "I began with a casual photo of black eyeglasses taken in a store, with a hand holding them and a busy background. It wasn‚Äôt suitable for an online listing, so I used an image-to-image model to refine it.\n",
        "\n",
        "With a prompt like ‚ÄúA realistic product photo of black eyeglasses isolated on a clean white background, studio lighting, no hands, no people, no background, centered, sharp focus‚Äù, the AI removed all distractions and created a clean, professional look. The glasses appeared crisp, well-lit, and perfectly centered‚Äîjust like a studio product photo.\n",
        "\n",
        "This transformation shows how image-to-image models can turn everyday photos into polished visuals ready for e-commerce, without the need for special equipment.\n",
        "\n",
        "<img src=\"objeto.jpg\" alt=\"My glasses \" width=\"300\" height=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5ea88862-d655-42f4-9717-85b599f2f73c",
      "metadata": {
        "id": "5ea88862-d655-42f4-9717-85b599f2f73c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c68025bf13c3408593312446af4f20fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load an image to modify\n",
        "init_image = Image.open(\"objeto.jpg\").convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "\n",
        "\n",
        "# Run img2img\n",
        "image = pipe_2(\n",
        "    prompt=\"A realistic product photo of black eyeglasses isolated on a clean white background, studio lighting, no hands, no people, no background, centered, sharp focus\",\n",
        "    negative_prompt=\"hand, fingers, skin, person, background, shelves, store, reflections, blur\",\n",
        "    image=init_image,\n",
        "    strength=0.7,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=80\n",
        ").images[0]\n",
        "\n",
        "\n",
        "# Save or show result\n",
        "image.save(\"glasses.jpg\")\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd093c7c-5e90-465f-8770-895c2b6693ce",
      "metadata": {
        "id": "fd093c7c-5e90-465f-8770-895c2b6693ce"
      },
      "source": [
        "### The result\n",
        "<img src=\"glasses.jpg\" alt=\"My modified Wallet\" width=\"300\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7ff91f5-b1ee-4360-9da3-13f26b810bef",
      "metadata": {
        "id": "b7ff91f5-b1ee-4360-9da3-13f26b810bef"
      },
      "source": [
        "## 2.1 Improving my prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a242edee-8f7a-43d0-891e-a006344534d7",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "685bb207578847c8881298de378eff92",
            "1a28fe6538b24684b370541bd605221d"
          ]
        },
        "id": "a242edee-8f7a-43d0-891e-a006344534d7",
        "outputId": "b4781902-bd1a-47b2-c0ea-fed36259d056"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53bf568f2da9464db5891b73b5cc8183",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load an image to modify\n",
        "init_image = Image.open(\"objeto.jpg\").convert(\"RGB\").resize((512, 512))\n",
        "\n",
        "image = pipe_2(\n",
        "    prompt=\"A realistic product photo of the same black eyeglasses, isolated on a clean white background, studio lighting, sharp focus, accurate shape and proportions, photorealistic\",\n",
        "    negative_prompt=\"hand, fingers, skin, person, background, shelves, store, clutter, blur, noise, distortion, stylized\",\n",
        "    image=init_image,\n",
        "    strength=0.45,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=80\n",
        ").images[0]\n",
        "\n",
        "\n",
        "# Save or show result\n",
        "image.save(\"eyeglasses with a optimized prompt.jpg\")\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec11bb95-1f55-4a55-a010-063822b02063",
      "metadata": {
        "id": "ec11bb95-1f55-4a55-a010-063822b02063"
      },
      "source": [
        "### The result\n",
        "<img src=\"eyeglasses with a optimized prompt.jpg\" alt=\"wallet without background with a optimized prompt.jpg\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ea9d7c-cd81-4ab6-b1cf-601f378b24e0",
      "metadata": {
        "id": "64ea9d7c-cd81-4ab6-b1cf-601f378b24e0"
      },
      "source": [
        "## üé• Creating My futuristic space station in a Video\n",
        "\n",
        "A visual sequence of a futuristic space station was generated using a text-based diffusion model. The process started from a detailed prompt defining a realistic, cinematic scene, and subsequent frames were produced by gradually varying lighting conditions throughout the day. This approach preserved visual consistency while creating a smooth temporal transition, resulting in a short video that simulates the passage of time in an orbital environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "54074318",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff8ba43d121340bf8f9cc8e7e8dce137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1770718f8097413f9e16b0ab472a8bef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando frame 1 - Prompt: Futuristic space station at morning, soft sunlight, cinematic\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00368145785743809cb2aeef2a2748fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando frame 2 - Prompt: Futuristic space station at noon, strong sunlight, cinematic\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e4e1ffe7eb64e52b1c3654996148e5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando frame 3 - Prompt: Futuristic space station at evening, warm light, cinematic\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7dff430a9112441bbe08eef36ad1bafa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando frame 4 - Prompt: Futuristic space station at sunset, orange glow, cinematic\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2d1d0f7dd0647a4a258e4c59096d0e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando frame 5 - Prompt: Futuristic space station at night, Earth lights visible, cinematic\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7aa1337c1942488ebfb13b20a4feac89",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\57323\\AppData\\Local\\Temp\\ipykernel_22488\\3848296808.py:73: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  frame1 = imageio.imread(f\"frames/frame_{i:03d}.png\")\n",
            "C:\\Users\\57323\\AppData\\Local\\Temp\\ipykernel_22488\\3848296808.py:74: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  frame2 = imageio.imread(f\"frames/frame_{i+1:03d}.png\")\n",
            "C:\\Users\\57323\\AppData\\Local\\Temp\\ipykernel_22488\\3848296808.py:81: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  frames.append(imageio.imread(f\"frames/frame_{len(prompts):03d}.png\"))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "pipe_1 = StableDiffusionPipeline.from_pretrained(\n",
        "    \"sd-legacy/stable-diffusion-v1-5\"\n",
        ")\n",
        "\n",
        "# Paso 4: Crear una carpeta para guardar las im√°genes\n",
        "os.makedirs(\"frames\", exist_ok=True)\n",
        "\n",
        "# 4. Imagen inicial desde texto\n",
        "initial_prompt = (\n",
        "    \"Ultra realistic futuristic space station at sunrise, \"\n",
        "    \"cinematic lighting, wide angle, stable composition, \"\n",
        "    \"high detail, sharp focus, realistic reflections, 8k\"\n",
        ")\n",
        "\n",
        "init_image = pipe_1(\n",
        "    initial_prompt,\n",
        "    strength=0.75,\n",
        "    guidance_scale=7.5,\n",
        "    num_inference_steps=50\n",
        ").images[0]\n",
        "\n",
        "init_image.save(\"frames/frame_000.png\")\n",
        "\n",
        "# 5. Prompts interpolados (misma escena, cambia la luz)\n",
        "prompts = [\n",
        "    \"Futuristic space station at morning, soft sunlight, cinematic\",\n",
        "    \"Futuristic space station at noon, strong sunlight, cinematic\",\n",
        "    \"Futuristic space station at evening, warm light, cinematic\",\n",
        "    \"Futuristic space station at sunset, orange glow, cinematic\",\n",
        "    \"Futuristic space station at night, Earth lights visible, cinematic\"\n",
        "]\n",
        "\n",
        "# 6. Generar frames fluidos con img2img\n",
        "prev_image = init_image\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"Generando frame {i+1} - Prompt: {prompt}\")\n",
        "\n",
        "    image = prev_image.resize((512, 512)).convert(\"RGB\")\n",
        "\n",
        "    new_image = pipe_1(\n",
        "        prompt=prompt,\n",
        "        image=image,\n",
        "        strength=0.6,          # mismo valor que ya usabas\n",
        "        guidance_scale=7.5\n",
        "    ).images[0]\n",
        "\n",
        "    frame_name = f\"frames/frame_{i+1:03d}.png\"\n",
        "    new_image.save(frame_name)\n",
        "\n",
        "    prev_image = new_image\n",
        "\n",
        "\n",
        "def interpolate_frames(img1, img2, steps=5):\n",
        "    img1 = Image.fromarray(img1)\n",
        "    img2 = Image.fromarray(img2)\n",
        "    return [\n",
        "        Image.blend(img1, img2, alpha=i / steps)\n",
        "        for i in range(1, steps)\n",
        "    ]\n",
        "\n",
        "\n",
        "frames = []\n",
        "for i in range(len(prompts)):\n",
        "    frame1 = imageio.imread(f\"frames/frame_{i:03d}.png\")\n",
        "    frame2 = imageio.imread(f\"frames/frame_{i+1:03d}.png\")\n",
        "\n",
        "    frames.append(frame1)\n",
        "    transition_frames = interpolate_frames(frame1, frame2, steps=5)\n",
        "    frames.extend([np.array(f) for f in transition_frames])\n",
        "\n",
        "# A√±adir el √∫ltimo frame\n",
        "frames.append(imageio.imread(f\"frames/frame_{len(prompts):03d}.png\"))\n",
        "\n",
        "imageio.mimsave(\"futuristic_space_station_video.mp4\", frames, fps=10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5536b92-a637-41e5-8c4a-427410bfeb16",
      "metadata": {
        "id": "c5536b92-a637-41e5-8c4a-427410bfeb16"
      },
      "source": [
        "## üåê Other Resources about Stable Diffusion (Beyond Hugging Face)\n",
        "\n",
        "### üß† 1. [Stability AI (Official)](https://stability.ai)\n",
        "- Official creators of Stable Diffusion.\n",
        "- Offers research papers, blog posts, updates, and commercial tools.\n",
        "\n",
        "---\n",
        "\n",
        "### üñºÔ∏è 2. [DreamStudio](https://dreamstudio.ai)\n",
        "- Web-based tool developed by Stability AI.\n",
        "- Generate images from text with adjustable parameters (steps, guidance scale, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ 3. GitHub Repositories\n",
        "- [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion): The original open-source release.\n",
        "- [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui): Popular web UI for running Stable Diffusion locally with extensive features.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö 4. Research Papers\n",
        "- [Original Paper on arXiv](https://arxiv.org/abs/2112.10752): *High-Resolution Image Synthesis with Latent Diffusion Models*.\n",
        "- Explore new innovations like SDXL, ControlNet, and LoRA via [arXiv.org](https://arxiv.org).\n",
        "\n",
        "---\n",
        "\n",
        "### üåê 5. [Civitai](https://civitai.com)\n",
        "- Community platform for sharing and downloading custom models, LoRAs, embeddings, and fine-tuned styles for Stable Diffusion.\n",
        "\n",
        "---\n",
        "\n",
        "### üß∞ 6. [Runway ML](https://runwayml.com)\n",
        "- User-friendly platform for creative AI tools.\n",
        "- Offers text-to-image, image-to-video, and more, including Stable Diffusion-based models.\n",
        "\n",
        "---\n",
        "\n",
        "### üí¨ 7. Community Forums\n",
        "- [r/StableDiffusion](https://www.reddit.com/r/StableDiffusion/)\n",
        "- [r/StableDiffusionAI](https://www.reddit.com/r/StableDiffusionAI/)\n",
        "- Many GitHub and tool websites link to their own Discord servers for support and discussion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e707e4-f042-4a1f-a93a-20e65992c08c",
      "metadata": {
        "id": "f4e707e4-f042-4a1f-a93a-20e65992c08c"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Stability AI, ‚ÄúStability.Ai,‚Äù Stability.Ai, 2024. https://stability.ai/\n",
        "2. Hugging Face, ‚ÄúHugging Face ‚Äì On a mission to solve NLP, one commit at a time.,‚Äù huggingface.co, 2024. https://huggingface.co/\n",
        "3. Render Realm, ‚ÄúStable Diffusion explained (in less than 10 minutes),‚Äù YouTube, Mar. 29, 2024. https://www.youtube.com/watch?v=QdRP9pO89MY (accessed Nov. 26, 2024).\n",
        "4. ‚Äútimbrooks/instruct-pix2pix ¬∑ Hugging Face,‚Äù Huggingface.co, 2025. https://huggingface.co/timbrooks/instruct-pix2pix (accessed Jun. 03, 2025).\n",
        "‚Äå"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
